"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook=globalThis.webpackChunkphysical_ai_humanoid_robotics_textbook||[]).push([[469],{1555:(n,o,e)=>{e.r(o),e.d(o,{assets:()=>r,contentTitle:()=>c,default:()=>u,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"vision-language-action-conversational-robotics","title":"Vision-Language-Action (VLA) & Conversational Robotics","description":"This chapter focuses on the convergence of large language models (LLMs) and robotics, enabling sophisticated voice-to-action capabilities. Students will integrate GPT models for conversational AI in robots, covering speech recognition, natural language understanding, and multi-modal interaction.","source":"@site/docs/vision-language-action-conversational-robotics.md","sourceDirName":".","slug":"/vision-language-action-conversational-robotics","permalink":"/hackathon_book/vision-language-action-conversational-robotics","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"id":"vision-language-action-conversational-robotics","title":"Vision-Language-Action (VLA) & Conversational Robotics","sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Humanoid Robot Development","permalink":"/hackathon_book/humanoid-robot-development"},"next":{"title":"Part 4: Appendices & Projects","permalink":"/hackathon_book/category/part-4-appendices--projects"}}');var t=e(4848),a=e(8453);const s={id:"vision-language-action-conversational-robotics",title:"Vision-Language-Action (VLA) & Conversational Robotics",sidebar_position:6},c="Vision-Language-Action (VLA) & Conversational Robotics",r={},l=[{value:"Module 4: Vision-Language-Action (VLA)",id:"module-4-vision-language-action-vla",level:2},{value:"Weekly Breakdown (Week 13): Conversational Robotics",id:"weekly-breakdown-week-13-conversational-robotics",level:2}];function d(n){const o={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(o.header,{children:(0,t.jsx)(o.h1,{id:"vision-language-action-vla--conversational-robotics",children:"Vision-Language-Action (VLA) & Conversational Robotics"})}),"\n",(0,t.jsx)(o.p,{children:"This chapter focuses on the convergence of large language models (LLMs) and robotics, enabling sophisticated voice-to-action capabilities. Students will integrate GPT models for conversational AI in robots, covering speech recognition, natural language understanding, and multi-modal interaction."}),"\n",(0,t.jsx)(o.h2,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"}),"\n",(0,t.jsxs)(o.ul,{children:["\n",(0,t.jsxs)(o.li,{children:[(0,t.jsx)(o.strong,{children:"Focus:"})," The convergence of LLMs and Robotics."]}),"\n",(0,t.jsx)(o.li,{children:"Voice-to-Action: Using OpenAI Whisper for voice commands."}),"\n",(0,t.jsx)(o.li,{children:'Cognitive Planning: Using LLMs to translate natural language ("Clean the room") into a sequence of ROS 2 actions.'}),"\n",(0,t.jsx)(o.li,{children:"Capstone Project: The Autonomous Humanoid. A final project where a simulated robot receives a voice command, plans a path, navigates obstacles, identifies an object using computer vision, and manipulates it."}),"\n"]}),"\n",(0,t.jsx)(o.h2,{id:"weekly-breakdown-week-13-conversational-robotics",children:"Weekly Breakdown (Week 13): Conversational Robotics"}),"\n",(0,t.jsxs)(o.ul,{children:["\n",(0,t.jsx)(o.li,{children:"Integrating GPT models for conversational AI in robots"}),"\n",(0,t.jsx)(o.li,{children:"Speech recognition and natural language understanding"}),"\n",(0,t.jsx)(o.li,{children:"Multi-modal interaction: speech, gesture, vision"}),"\n"]})]})}function u(n={}){const{wrapper:o}={...(0,a.R)(),...n.components};return o?(0,t.jsx)(o,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}},8453:(n,o,e)=>{e.d(o,{R:()=>s,x:()=>c});var i=e(6540);const t={},a=i.createContext(t);function s(n){const o=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(o):{...o,...n}},[o,n])}function c(n){let o;return o=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),i.createElement(a.Provider,{value:o},n.children)}}}]);
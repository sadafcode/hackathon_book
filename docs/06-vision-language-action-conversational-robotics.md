---
id: 06-vision-language-action-conversational-robotics
title: Vision-Language-Action (VLA) & Conversational Robotics
sidebar_position: 6
---

# Vision-Language-Action (VLA) & Conversational Robotics

This chapter focuses on the convergence of large language models (LLMs) and robotics, enabling sophisticated voice-to-action capabilities. Students will integrate GPT models for conversational AI in robots, covering speech recognition, natural language understanding, and multi-modal interaction.

## Module 4: Vision-Language-Action (VLA)
- **Focus:** The convergence of LLMs and Robotics.
- Voice-to-Action: Using OpenAI Whisper for voice commands.
- Cognitive Planning: Using LLMs to translate natural language ("Clean the room") into a sequence of ROS 2 actions.
- Capstone Project: The Autonomous Humanoid. A final project where a simulated robot receives a voice command, plans a path, navigates obstacles, identifies an object using computer vision, and manipulates it.

## Weekly Breakdown (Week 13): Conversational Robotics
- Integrating GPT models for conversational AI in robots
- Speech recognition and natural language understanding
- Multi-modal interaction: speech, gesture, vision